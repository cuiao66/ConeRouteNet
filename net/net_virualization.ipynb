{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import math\n",
                "import copy\n",
                "import torch\n",
                "from torch import nn, Tensor\n",
                "import torch.nn.functional as F\n",
                "import torch.nn as nn\n",
                "from functools import partial\n",
                "import numpy as np\n",
                "import logging\n",
                "from typing import Optional, List\n",
                "from collections import OrderedDict\n",
                "from timm.models.registry import register_model\n",
                "from timm.models.resnet import resnet26d, resnet50d, resnet18d, resnet26, resnet50, resnet101d\n",
                "from timm.models import (\n",
                "    create_model,\n",
                "    safe_model_name,\n",
                "    resume_checkpoint,\n",
                "    load_checkpoint,\n",
                "    convert_splitbn_model,\n",
                "    model_parameters,\n",
                ")\n",
                "from Demo import DemoEnd2EndNet_baseline\n",
                "import os\n",
                "from timm.data import (\n",
                "    create_dataset,\n",
                "    create_loader,\n",
                "    resolve_data_config,\n",
                "    Mixup,\n",
                "    FastCollateMixup,\n",
                "    AugMixDataset,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "d:\\xm\\DemoEnd2EndNet\\venv\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3484.)\n",
                        "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
                    ]
                }
            ],
            "source": [
                "os.environ['project_dir'] = \"D:\\\\xm\\\\DemoEnd2EndNet\"\n",
                "model = create_model(\n",
                "        \"DemoEnd2EndNet_baseline\",\n",
                "        pretrained=False,\n",
                "        with_lidar=False,\n",
                "        only_encoder=True,\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "<All keys matched successfully>"
                        ]
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "model.load_state_dict(torch.load(r'D:\\xm\\DemoEnd2EndNet\\net\\output\\20230819-221257-DemoEnd2EndNet_baseline-224-less_dataset\\model_best.pth.tar')['state_dict'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "import PIL.Image as Image\n",
                "from PIL import ImageTransform"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "img_path = r\"E:\\CarlaDataSet\\weathers\\Town06_long\\routes_town06_long_w9_02_27_11_37_19\\rgb_front\\0764.jpg\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "img = Image.open(img_path).convert('RGB')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "img"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from torchvision import transforms\n",
                "transform = transforms.Compose([\n",
                "    # 重置大小\n",
                "    transforms.Resize(255),\n",
                "    transforms.CenterCrop(224),\n",
                "    # 随机旋转图片\n",
                "    transforms.RandomHorizontalFlip(),\n",
                "    transforms.ToTensor(),\n",
                "    # 正则化（降低模型复杂度）\n",
                "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
                "])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "img_tensor = transform(img)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x = img_tensor.unsqueeze(0)\n",
                "x = model.rgb_backbone.model.patch_embed(x)\n",
                "x = model.rgb_backbone.model.pos_drop(x)\n",
                "\n",
                "for level in model.rgb_backbone.model.levels:\n",
                "    x = level(x)\n",
                "\n",
                "x = model.rgb_backbone.model.norm(x)\n",
                "x = x.permute(0, 3, 1, 2)\n",
                "# x = model.rgb_backbone.model.avgpool(x)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "temp = torch.sum(x, dim=1).detach().numpy()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "temp.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "temp = np.uint8(255 * (temp[0] - temp.min()) / (temp.max() - temp.min()))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import cv2\n",
                "from  matplotlib import pyplot as plt\n",
                "img = cv2.imread(img_path)\n",
                "height, width, _ = img.shape  #读取输入图片的尺寸\n",
                "heatmap = cv2.applyColorMap(cv2.resize(temp, (width, height)), 1)  #CAM resize match input image size\n",
                "result = np.uint8(heatmap * 0.3 + img * 0.5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "result.dtype"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from  matplotlib import pyplot as plt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.imshow(result)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.imshow(result)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.imshow(result)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.imshow(result)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "result.max()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "params = model.brake_pred_head[0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "params.weight.detach().max()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "from timm.data import create_carla_dataset, create_carla_loader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset_train = create_carla_dataset(\n",
                "    'carla',\n",
                "    root='E:\\\\CarlaDataSet',\n",
                "    batch_size=1,\n",
                "    weathers=[1,2,3,4,5,6,7,8,9,10],\n",
                "\n",
                "    towns=[1, 2, 3, 4, 6, 10],\n",
                "    multi_view=True,\n",
                "    use_junction_data=True,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "loader_train = create_carla_loader(\n",
                "    dataset_train,\n",
                "    input_size=224,\n",
                "    batch_size=1,\n",
                "    is_training=True\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "iter = enumerate(loader_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "batch_idx, (input, target) = iter.__next__()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "output = model(input)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "x = input\n",
                "front_image = x[\"rgb\"]\n",
                "left_image = x[\"rgb_left\"]\n",
                "right_image = x[\"rgb_right\"]\n",
                "front_center_image = x[\"rgb_center\"]\n",
                "measurements = x[\"measurements\"]\n",
                "target_point = x[\"target_point\"]\n",
                "\n",
                "if model.direct_concat:\n",
                "    img_size = front_image.shape[-1]\n",
                "    left_image = torch.nn.functional.interpolate(\n",
                "        left_image, size=(img_size, img_size)\n",
                "    )\n",
                "    right_image = torch.nn.functional.interpolate(\n",
                "        right_image, size=(img_size, img_size)\n",
                "    )\n",
                "    front_center_image = torch.nn.functional.interpolate(\n",
                "        front_center_image, size=(img_size, img_size)\n",
                "    )\n",
                "    front_image = torch.cat(\n",
                "        [front_image, left_image, right_image, front_center_image], dim=1\n",
                "    )\n",
                "\n",
                "\n",
                "\n",
                "features = model.forward_features(\n",
                "    front_image,\n",
                "    left_image,\n",
                "    right_image,\n",
                "    front_center_image,\n",
                "    measurements,\n",
                ")\n",
                "\n",
                "bs = front_image.shape[0]\n",
                "\n",
                "\n",
                "\n",
                "tgt = model.position_encoding(\n",
                "    torch.ones((bs, 1, 20, 20), device=x[\"rgb\"].device)\n",
                ")\n",
                "tgt = tgt.flatten(2)\n",
                "tgt = torch.cat([tgt, model.query_pos_embed.repeat(bs, 1, 1)], 2)\n",
                "tgt = tgt.permute(2, 0, 1)\n",
                "\n",
                "features = torch.cat((features, model.waypoint_embed.repeat(1, bs, 1)), dim=0)\n",
                "memory = model.encoder(features, mask=model.attn_mask)\n",
                "\n",
                "hs = model.decoder(model.query_embed.repeat(1, bs, 1), memory, query_pos=tgt)[0]\n",
                "\n",
                "hs = hs.permute(1, 0, 2)  # Batchsize ,  N, C\n",
                "\n",
                "traffic_feature = hs[:, :400]\n",
                "\n",
                "velocity = measurements[:, 6:7].unsqueeze(-1)\n",
                "velocity = velocity.repeat(1, 400, 32)\n",
                "traffic_feature_with_vel = torch.cat([traffic_feature, velocity], dim=2)\n",
                "traffic = model.traffic_pred_head(traffic_feature_with_vel)\n",
                "\n",
                "memory = memory.permute(1, 0, 2)  # Batchsize ,  N, C\n",
                "\n",
                "is_junction_feature = memory[:, 11]\n",
                "traffic_light_state_feature = memory[:, 11]\n",
                "stop_sign_feature = memory[:, 11]\n",
                "brake_feature = torch.cat((memory, features.permute(1, 0, 2)), dim=1).permute(0, 2, 1)\n",
                "waypoints_feature = memory[:, 0:10]\n",
                "\n",
                "waypoints = model.waypoints_generator(waypoints_feature, target_point)\n",
                "\n",
                "\n",
                "is_junction = model.junction_pred_head(is_junction_feature)\n",
                "\n",
                "\n",
                "traffic_light_state = model.traffic_light_pred_head(traffic_light_state_feature)\n",
                "\n",
                "stop_sign = model.stop_sign_head(stop_sign_feature)\n",
                "\n",
                "# velocity = measurements[:, 6:7].unsqueeze(-1)\n",
                "# velocity = self.traffic_pred_head(velocity)\n",
                "\n",
                "brake_feature = model.brake_gap(brake_feature).squeeze(2)\n",
                "brake = model.brake_pred_head(brake_feature)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[9.9999e-01, 5.1604e-06]], grad_fn=<SoftmaxBackward0>)"
                        ]
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.softmax(brake, dim=1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[  0.8932, -11.2813]], grad_fn=<AddmmBackward0>)"
                        ]
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "brake"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "torch.Size([2, 7])"
                        ]
                    },
                    "execution_count": 25,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "model.brake_pred_head[4].weight.detach().shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "Sequential(\n",
                            "  (0): Linear(in_features=512, out_features=64, bias=True)\n",
                            "  (1): Dropout(p=0.5, inplace=False)\n",
                            "  (2): Linear(in_features=64, out_features=7, bias=True)\n",
                            "  (3): ReLU()\n",
                            "  (4): Linear(in_features=7, out_features=2, bias=True)\n",
                            ")"
                        ]
                    },
                    "execution_count": 35,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "model.brake_pred_head"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[-0.0393, -1.9285],\n",
                            "        [-0.1473,  1.4721],\n",
                            "        [-0.2925, -3.0395],\n",
                            "        [-0.2664, -1.9549],\n",
                            "        [ 0.0260,  3.8824],\n",
                            "        [-0.3308,  1.8526],\n",
                            "        [-0.2988,  1.2765]], grad_fn=<MulBackward0>)"
                        ]
                    },
                    "execution_count": 28,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "brake * model.brake_pred_head[4].weight.detach().T"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [],
            "source": [
                "leaner = model.brake_pred_head[4]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[-0.0426, -1.9565],\n",
                            "        [-0.1599,  1.4934],\n",
                            "        [-0.3175, -3.0835],\n",
                            "        [-0.2891, -1.9832],\n",
                            "        [ 0.0283,  3.9387],\n",
                            "        [-0.3590,  1.8795],\n",
                            "        [-0.3243,  1.2950]], grad_fn=<MulBackward0>)"
                        ]
                    },
                    "execution_count": 34,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "(brake - leaner.bias) * model.brake_pred_head[4].weight.detach().T"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor(0.4647, grad_fn=<MeanBackward0>)"
                        ]
                    },
                    "execution_count": 42,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "brake_feature.mean()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
        },
        "kernelspec": {
            "display_name": "Python 3.9.12 64-bit",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
